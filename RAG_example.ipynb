{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNTD5c7XICFJVO/+H9GF0dH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanessayanbingzhu/vanessayanbingzhu.github.io/blob/main/RAG_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain example\n",
        "\n",
        "We will use `LangChain` here as an example to show how to answer questions based on our personal data using LLM.\n",
        "\n",
        "Things we will do in this example:\n",
        "- Loading documents\n",
        "- Splitting loaded documents\n",
        "- Embedding and vector store\n",
        "- Retrieval\n",
        "- Answering questions\n"
      ],
      "metadata": {
        "id": "LgsCYNz5WtM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing `Python` packages\n",
        "\n",
        "Note that package version is important. Please install the right version to meet your need.\n",
        "Below is the python package version we will use in this example."
      ],
      "metadata": {
        "id": "eOPej6J_ZE54"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ynYITU2WeOP"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain==0.0.235 openai==0.28.1 chromadb==0.4.14 pypdf pymupdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "# from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
        "from langchain.llms import OpenAI, OpenAIChat\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "import openai"
      ],
      "metadata": {
        "id": "mxvUfYJ0Znn2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.Setting up OpenAI environment\n",
        "\n",
        "Here use API key from OpenAI. Input your own API key when running the code."
      ],
      "metadata": {
        "id": "ovgVb_ExfW8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "qos8ciNLfSjh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Loading documents\n",
        "\n",
        "Royal Bank of Canada 2023 annual report pdf file is used as an example here."
      ],
      "metadata": {
        "id": "nwHjD6-OgLYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_NAME = \"RBC_2023_annaul_report.pdf\"\n",
        "loader = PyPDFLoader(PDF_NAME)\n",
        "docs = loader.load()\n",
        "\n",
        "print (f'There are {len(docs)} document(s) in {PDF_NAME}.')\n",
        "print (f'There are {len(docs[0].page_content)} characters in the first page of your document.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eLBfZORhorT",
        "outputId": "30b6ba3b-9485-44a0-86ef-c52594a6a1f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 237 document(s) in RBC_2023_annaul_report.pdf.\n",
            "There are 76 characters in the first page of your document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.Splitting loaded documents\n",
        "\n",
        "Documents need to be splitted into smaller chunks before it goes into vector store. How we split the data is important and could be tricky.\n",
        "\n",
        "Chunk size is a length function measuring the size of the chunk. Chunk overlap is to have overlap between two chunks and allows for consistency."
      ],
      "metadata": {
        "id": "RqQzbteEiOsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_docs = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "9Y9e4ZWzgLA_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.Embedding and vector stores.\n",
        "\n",
        "Once we split our documents into small chunks, we need to vectorize them so that we can easily retrieve them when answering questions based on our data.\n",
        "\n",
        "`Embedding` is a mathematical representation of a set of data points in lower dimension space. It captures the underlying relationships and patterns of the data. Texts with similar content will have similar vectors. Therefore we can find texts that are similar based on embedding. After that we store the embeddings in the vector store.\n",
        "\n",
        "Chroma is used as the vector store in this example."
      ],
      "metadata": {
        "id": "uYJelNXykJq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(split_docs, embeddings, collection_name=\"serverless_guide\")"
      ],
      "metadata": {
        "id": "GsAtQTJKkeAU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.Retrieval"
      ],
      "metadata": {
        "id": "AhYLYalTnqJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why RBC's total revenue increase in 2023 comparing to 2022?\""
      ],
      "metadata": {
        "id": "5LgZv0PTovLQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity search**"
      ],
      "metadata": {
        "id": "bsybdy69qAel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_docs = vectorstore.similarity_search(query)\n",
        "print(similar_docs[0].page_content)"
      ],
      "metadata": {
        "id": "qhAdlOYGoyWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximum Marginal Relevance (MMR)**"
      ],
      "metadata": {
        "id": "LfdmGzPurFVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_mmr = vectorstore.max_marginal_relevance_search(query)\n",
        "print(docs_mmr[0].page_content)"
      ],
      "metadata": {
        "id": "LQq6tcbhrcnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.Answering questions\n",
        "\n",
        "Answering questions baed on QA chain.\n",
        "\n",
        "Temperature is how \"creative\" we want the answers to be. We set it to 0 initially to lower their variability."
      ],
      "metadata": {
        "id": "UOmP8equsHei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "#similarity search\n",
        "chain.run(input_documents=similar_docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OukQsZ-FkD2_",
        "outputId": "d2917a54-0432-4949-c708-5edda62be132"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RBC's total revenue increased in 2023 compared to 2022 mainly due to higher net interest income, insurance premiums, investment and fee income, trading revenue, and other sources of revenue.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RetrievalQA**"
      ],
      "metadata": {
        "id": "sULZoxw42PKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectorstore.as_retriever()\n",
        "    )"
      ],
      "metadata": {
        "id": "kuZDs1JBtguB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain({\"query\": query})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ceWhM5Jyt8yj",
        "outputId": "829ad3dc-639a-4770-bae3-b0b1e8be60c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RBC's total revenue increased in 2023 compared to 2022 mainly due to higher net interest income, insurance premiums, investment and fee income, trading revenue, and other revenue sources.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RetrievalQA using prompt template**"
      ],
      "metadata": {
        "id": "O4dWuigH06Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum. Keep the answer as concise as possible.\n",
        "Always say \"Thank you for your question!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\n",
        "\"\"\"\n",
        "\n",
        "QAchain_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "qa_chain_pmt = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QAchain_prompt}\n",
        ")"
      ],
      "metadata": {
        "id": "Y7603HruuBFn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_pmt = qa_chain_pmt({\"query\": query})\n",
        "result_pmt[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Fz70E8GvuYF8",
        "outputId": "4ca44dda-b676-4071-cbfe-b8659f153eac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RBC's total revenue increased in 2023 compared to 2022 mainly due to higher net interest income, insurance premiums, investment and fee income, trading revenue, and other revenue sources. Factors such as higher investment management and custodial fees, foreign exchange revenue, and insurance revenue contributed to the increase. Thank you for your question!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iG3xv-IQukOW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}